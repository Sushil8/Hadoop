require(devtools)
library(plyr)
library(httr)
library(doBy)
library(Quandl)
library(twitteR)
require(twitteR)
library(devtools)
require(base64enc)
require(wordcloud)
require(tm)
require(rvest)
library(stringr)
library(doBy)
require(devtools)
library(plyr)
library(httr)
library(doBy)
library(Quandl)
library(twitteR)
require(twitteR)
library(devtools)
require(base64enc)
require(wordcloud)
require(tm)
require(rvest)
library(stringr)
library(doBy)
require(devtools)
library(plyr)
library(httr)
library(doBy)
library(Quandl)
library(twitteR)
require(twitteR)
library(devtools)
require(base64enc)
require(wordcloud)
require(tm)
require(rvest)
library(stringr)
library(doBy)
api_key <- "R2rnDKtqJ4PV5UzgC4gPtLRDj"
api_secret <- "4HWrYEInbwYzPAvWnRtt89el3Ovh2obucdb6TUYJ7FdtfVTZNU"
access_token <- "567367655-dDImmurR3DwXJaMzxsWSHcG0zua0dpgR87w1m4Hu"
access_token_secret <- "e1u8svdkgDg9Ra2xVbHA3SxztmSYw4VnlpVe6MMeRRaPZ"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
Sud_negative = scan('C:\\Users\\Admin\\Documents\\negative-words.txt', what='character',comment.char=';')
Sud_positive = scan('C:\\Users\\Admin\\Documents\\Positive-words.txt', what='character',comment.char=';')
positive_words = c(Sud_positive, 'nice','rich','favorite','recommended','huge','awesome','good','favorite','excellent','comfortable','recommended')
negative_words = c(Sud_negative, 'bad', 'slow', 'tiring','terrible','fake','glorified','not recommended', 'wtf', 'boring', 'douzy', 'hilarious','terrible','slow','bad')
score.sentence <- function(sentence, positive_words, negative_words) {
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl::]]', '', sentence)
sentence = gsub('\\d+', '', sentence)
sentence = tolower(sentence)
word.list = str_split(sentence, '\\s+')
words = unlist(word.list)
positive_matches = match(words, positive_words)
negative_matches = match(words, negative_words)
positive_matches = !is.na(positive_matches)
negative_matches = !is.na(negative_matches)
score = sum(positive_matches) - sum(negative_matches)
return(score)
}
score.sentiana <- function(sentences, positive_words, negative_words) {
require(plyr)
require(stringr)
scores = laply(sentences, function(sentence, positive_words, negative_words) {
tryCatch(score.sentence(sentence, positive_words, negative_words ), error=function(e) 0)
}, positive_words, negative_words)
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
prepare_score <- function (handle, City, positive_words, negative_words) {
tweets = searchTwitter(handle, n=1000, lang="en", since=NULL, retryOnRateLimit=10)
text = laply(tweets, function(t) t$getText())
score = score.sentiana(text, positive_words, negative_words)
score$City = City
return (score)
}
city1 = prepare_score("Munich", "Munich", positive_words, negative_words)
City2=prepare_score("Perth","Perth",positive_words,negative_words)
City3=prepare_score("Luxembourg","Luxembourg",positive_words,negative_words)
City4=prepare_score("Ottawa","Ottawa",positive_words,negative_words)
City5=prepare_score("Doha","Doha",positive_words,negative_words)
City6=prepare_score("Sydney","Sydney",positive_words,negative_words)
City7=prepare_score("Melbourne","Melbourne",positive_words,negative_words)
City8=prepare_score("Brisbane","Brisbane",positive_words,negative_words)
City9=prepare_score("Washington","Washington",positive_words,negative_words)
City10=prepare_score("Boston","Boston",positive_words,negative_words)
City11=prepare_score("Dubai","Dubai",positive_words,negative_words)
City12=prepare_score("Dusseldorf","Dusseldorf",positive_words,negative_words)
City13=prepare_score("Atlanta","Atlanta",positive_words,negative_words)
City14=prepare_score("Hong Kong","Hong Kong",positive_words,negative_words)
City15=prepare_score("Auckland","Auckland",positive_words,negative_words)
City16=prepare_score("Dallas","Dallas",positive_words,negative_words)
City17=prepare_score("Chicago","Chicago",positive_words,negative_words)
City18=prepare_score("Tokyo","Tokyo",positive_words,negative_words)
api_key <- "l8lp8c1fYibGPEcCteHI73Y6B"
api_secret <- "XxYlM5jJ1A6eY7C1pUitRaVrYbAkrcuXH93tRDGhIopZGL7DO5"
access_token <- "267106985-iCYqablUwOsATT4AOaVLpP7BgiX53j4h1GDzmWpE"
access_token_secret <- "DfdT2CmA28ZA1XEYzY1X5vuNTKfuZs0N0gGhrPwwgwLfw"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
City18=prepare_score("Tokyo","Tokyo",positive_words,negative_words)
City19=prepare_score("Miami","Miami",positive_words,negative_words)
City20=prepare_score("Paris","Paris",positive_words,negative_words)
City21=prepare_score("Munich","Munich",positive_words,negative_words)
City22=prepare_score("London","London",positive_words,negative_words)
City23=prepare_score("Dublin","Dublin",positive_words,negative_words)
City24=prepare_score("Galway","Galway",positive_words,negative_words)
City25=prepare_score("Frankfurt","Frankfurt",positive_words,negative_words)
City26=prepare_score("Venice","Venice",positive_words,negative_words)
City27=prepare_score("Malmo","Malmo",positive_words,negative_words)
City28=prepare_score("Berlin","Berlin",positive_words,negative_words)
City29=prepare_score("Amsterdam","Amsterdam",positive_words,negative_words)
City30=prepare_score("Glasgow","Glasgow",positive_words,negative_words)
City31=prepare_score("Vienna","Vienna",positive_words,negative_words)
City32=prepare_score("Singapore","Singapore",positive_words,negative_words)
City33=prepare_score("Liverpool","Liverpool",positive_words,negative_words)
City34=prepare_score("Belfast","Belfast",positive_words,negative_words)
City35=prepare_score("Milan","Milan",positive_words,negative_words)
City35=prepare_score("Milan","Milan",positive_words,negative_words)
City36=prepare_score("Christchurch","Christchurch",positive_words,negative_words)
City37=prepare_score("Manchester","Manchester",positive_words,negative_words)
City38=prepare_score("Madrid","Madrid",positive_words,negative_words)
City39=prepare_score("Rome","Rome",positive_words,negative_words)
City40=prepare_score("Barcelona","Barcelona",positive_words,negative_words)
City41=prepare_score("Valencia","Valencia",positive_words,negative_words)
City42=prepare_score("Moscow","Moscow",positive_words,negative_words)
City43=prepare_score("Prague","Prague",positive_words,negative_words)
City44=prepare_score("Beijing","Beijing",positive_words,negative_words)
City45=prepare_score("Mexico","Mexico",positive_words,negative_words)
City46=prepare_score("Bangalore","Bangalore",positive_words,negative_words)
City47=prepare_score("Budapest","Budapest",positive_words,negative_words)
City48=prepare_score("Delhi","Delhi",positive_words,negative_words)
City49=prepare_score("Pattaya","Pattaya",positive_words,negative_words)
City50=prepare_score("Mumbai","Mumbai",positive_words,negative_words)
City51=prepare_score("Jakarta","Jakarta",positive_words,negative_words)
City52=prepare_score("Chennai","Chennai",positive_words,negative_words)
api_key <- "R2rnDKtqJ4PV5UzgC4gPtLRDj"
api_secret <- "4HWrYEInbwYzPAvWnRtt89el3Ovh2obucdb6TUYJ7FdtfVTZNU"
access_token <- "567367655-dDImmurR3DwXJaMzxsWSHcG0zua0dpgR87w1m4Hu"
access_token_secret <- "e1u8svdkgDg9Ra2xVbHA3SxztmSYw4VnlpVe6MMeRRaPZ"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
City52=prepare_score("Chennai","Chennai",positive_words,negative_words)
City53=prepare_score("Johannesburg","Johannesburg",positive_words,negative_words)
City54=prepare_score("Los Angeles","Los Angeles",positive_words,negative_words)
City55=prepare_score("Istanbul","Istanbul",positive_words,negative_words)
City56=prepare_score("Bangkok","Bangkok",positive_words,negative_words)
City57=prepare_score("Cork","Cork",positive_words,negative_words)
City58=prepare_score("Oxford","Oxford",positive_words,negative_words)
City59=prepare_score("Hamilton","Hamilton",positive_words,negative_words)
City60=prepare_score("Limerick","Limerick",positive_words,negative_words)
City61=prepare_score("Florence","Florence",positive_words,negative_words)
City62=prepare_score("Toronto","Toronto",positive_words,negative_words)
City63=prepare_score("Newcastle","Newcastle",positive_words,negative_words)
City64=prepare_score("Phuket","Phuket",positive_words,negative_words)
complete.scores= rbind(City1,City2,City3,City4,City5,City6,City7,City8,City9,City10,City11,City12,City13,City14,City15,City16,City17,City18,City19,City20,City21,City22,City23,City24,City25,City26,City27,City28,City29,City30,City31,City32,City33,City34,City35,City36,City37,City38,City39,City40,City41,City42,City43,City44,City45,City46,City47,City48,City49,City50,City51,City52,City53,City54,City55,City56,City57,City58,City59,City60,City61,City62,City63,City64)
complete.scores= rbind(city1,City2,City3,City4,City5,City6,City7,City8,City9,City10,City11,City12,City13,City14,City15,City16,City17,City18,City19,City20,City21,City22,City23,City24,City25,City26,City27,City28,City29,City30,City31,City32,City33,City34,City35,City36,City37,City38,City39,City40,City41,City42,City43,City44,City45,City46,City47,City48,City49,City50,City51,City52,City53,City54,City55,City56,City57,City58,City59,City60,City61,City62,City63,City64)
complete.scores$very.pos = as.numeric( complete.scores$score >= 2)
complete.scores$very.neg = as.numeric( complete.scores$score <= -2)
#near 0
complete.scores$very.pos = as.numeric( complete.scores$score >= 1)
complete.scores$score = as.numeric( complete.scores$score <= -1)
#the pos/neg sentiment scores for different companies
twitter.df = ddply(complete.scores,c('City'), summarise, pos.count = sum (very.pos), neg.count = sum(very.neg))
twitter.df$complete.count = twitter.df$pos.count + twitter.df$neg.count
#sentiment score to be a percentage
twitter.df$score = round (100 * twitter.df$pos.count / twitter.df$complete.count)
orderBy(~-score, twitter.df)
write.csv(orderBy(~-score, twitter.df),file='Sentiment.csv')
iconv(KukÃ«s, “LATIN2”, UTF-8”)
iconv("KukÃ«s", “LATIN2”, UTF-8”)
var <- iconv("KukÃ«s", “LATIN2”, UTF-8”)
var <- iconv("KukÃ«s", “LATIN2”, UTF-8”)
mydata <- read.csv(file = "C:\Users\Admin\Desktop\simplemaps-worldcities-basic.csv", header =TRUE,sep= ",")
getwd
getwd()
mydata <- read.csv(file = "simplemaps-worldcities-basic.csv", header =TRUE,sep= ",")
mydata$province <- NULL
mydata$iso2 <- NULL
mydata$iso3 <- NULL
data[!grepl(pattern = "[^A-Za-z()-]", x = mydata)]
data[!grepl(pattern = "[^A-Za-z()]", x = mydata)]
dta[!grepl(pattern = "[^A-Za-z()]", x = mydata)]
s <- data[!grepl(pattern = "[^A-Za-z()]", x = mydata)]
mydata[!grepl(pattern = "[^A-Za-z()]", x = mydata)]
library(stringr)
library(stringr)
str_replace_all(mydata, "[^[:alnum:]]", "", mydata)
str_replace_all(mydata, "[^[:alnum:]]", "")
mydata
write.csv(mydata, "simplemaps.csv")
str_replace_all(mydata$city, "[^[:alnum:]]", "")
write.csv(mydata, "simplemaps.csv")
typeof(mydata)
print(names)
for name in names(mydata){
print(names)
}
for (name in names(mydata)){
print(names)
}
print(name)
print(mydata[[name]])
mydata <- gsub("[^0-9A-Za-z///' ]","'" , mydata ,ignore.case = TRUE)
write.csv(mydata, "simplemaps.csv")
write.csv(mydata, "simplemaps.csv")
library(stringr)
mydata <- read.csv(file = "simplemaps-worldcities-basic.csv", header = TRUE, sep = ",")
mydata$iso2 <- NULL
mydata$iso3 <- NULL
mydata$province <- NULL
mydata <- gsub("[^0-9A-Za-z///' ]","'" , mydata ,ignore.case = TRUE)
write.csv(mydata, "simplemaps.csv")
rm(mydata)
rm(name)
library(stringr)
mydata <- read.csv(file = "simplemaps-worldcities-basic.csv", header = TRUE, sep = ",")
mydata$iso2 <- NULL
mydata$iso3 <- NULL
mydata$province <- NULL
mydata <- gsub("[^0-9A-Za-z///' ]","'" , mydata ,ignore.case = TRUE)
library(stringr)
mydata <- read.csv(file = "simplemaps-worldcities-basic.csv", header = TRUE, sep = ",")
mydata$iso2 <- NULL
mydata$iso3 <- NULL
mydata$province <- NULL
mydata$city <- gsub("[^0-9A-Za-z///' ]","'" , mydata$city ,ignore.case = TRUE)
write.csv(mydata, "simplemaps.csv")
mydata$city <- gsub("''","" , mydata$city ,ignore.case = TRUE)
write.csv(mydata, "simplemaps.csv")
mydata$city <- gsub("''","" , mydata$city ,ignore.case = TRUE)
write.csv(mydata, "simplemaps.csv")
library(stringr)
mydata <- read.csv(file = "simplemaps-worldcities-basic.csv", header = TRUE, sep = ",")
mydata$iso2 <- NULL
mydata$iso3 <- NULL
mydata$province <- NULL
mydata$city <- gsub("[^0-9A-Za-z///' ]","'" , mydata$city ,ignore.case = TRUE)
mydata$city <- gsub("''","" , mydata$city ,ignore.case = TRUE)
write.csv(mydata, "simplemaps.csv")
list2df
?list2df
??list2df
library(devtools)
list2df
install.packages("qdapTools")
library(qdapTools)
library(qdapTools)
library(readxl) #Load readxl package
library(rvest) #Load the rvest package
library(xml2)
url <- "https://www.numbeo.com/quality-of-life/rankings.jsp"
page <- read_html(url) #Downloads the URLs webpage
table <- html_table(page, fill = TRUE)
typeof(table)
write.csv(table,"city.csv")
mydata <- read.csv(file = "city.csv", header = TRUE, sep = ",")
typeof(mydata)
mydata$X <- NULL
mydata$X1 <- NULL
mydata$X2 <- NULL
mydata$X1.1 <- NULL
mydata$X2.1 <- NULL
mydata$X3 <- NULL
mydata$Rank <- NULL
View(mydata)
mydata$
list2df(mydata, col1 = "Rank", col2 ="City")
list2df(mydata, col1 = "Rank", col2 ="City")
write.csv(mydata, "mydata.csv")
typeof(mydata)
list2df(mydata, col1 = "Rank", col2 ="City")
library(readxl) #Load readxl package
library(rvest) #Load the rvest package
url <- "https://www.numbeo.com/quality-of-life/rankings.jsp"
page <- read_html(url) #Downloads the URLs webpage
table <- html_table(page, fill = TRUE)
write.csv(table,"city.csv")
mydata <- read.csv(file = "city.csv", header = TRUE, sep = ",")
mydata$X <- NULL
mydata$X1 <- NULL
mydata$X2 <- NULL
mydata$X1.1 <- NULL
mydata$X2.1 <- NULL
mydata$X3 <- NULL
mydata$Rank <- NULL
write.csv(mydata, "mydata.csv")
library(stringr)
url <- "https://www.numbeo.com/quality-of-life/rankings.jsp"
page <- read_html(url) #Downloads the URLs webpage
table <- html_table(page, fill = TRUE)
typeof(table)
write.csv(table,"city.csv")
mydata <- read.csv(file = "city.csv", header = TRUE, sep = ",")
typeof(mydata)
mydata$X <- NULL
mydata$X1 <- NULL
mydata$X2 <- NULL
mydata$X1.1 <- NULL
mydata$X2.1 <- NULL
mydata$X3 <- NULL
mydata$Rank <- NULL
mydata$city <- gsub(",.*","" , mydata$city ,ignore.case = TRUE)
write.csv(mydata, "mydata.csv")
write.csv(mydata, "mydata.csv")
mydata$city <- gsub(",*","" , mydata$city ,ignore.case = TRUE)
mydata$city <- gsub(",*","" , mydata$City ,ignore.case = TRUE)
write.csv(mydata, "mydata.csv")
url <- "https://www.numbeo.com/quality-of-life/rankings.jsp"
page <- read_html(url) #Downloads the URLs webpage
table <- html_table(page, fill = TRUE)
write.csv(table,"city.csv")
mydata$X <- NULL
mydata$X1 <- NULL
mydata$X2 <- NULL
mydata$X1.1 <- NULL
mydata$X2.1 <- NULL
mydata$X3 <- NULL
mydata$Rank <- NULL
mydata
mydata <- read.csv(file = "city.csv", header = TRUE, sep = ",")
mydata$X <- NULL
mydata$X1 <- NULL
mydata$X2 <- NULL
mydata$X1.1 <- NULL
mydata$X2.1 <- NULL
mydata$X3 <- NULL
mydata$Rank <- NULL
mydata$City <- gsub(",*","", mydata$City, ignore.case = TRUE)
write.csv(mydata, "mydata.csv")
library(stringr)
url <- "https://www.numbeo.com/quality-of-life/rankings.jsp"
page <- read_html(url) #Downloads the URLs webpage
table <- html_table(page, fill = TRUE)
typeof(table)
write.csv(table,"city.csv")
mydata <- read.csv(file = "city.csv", header = TRUE, sep = ",")
typeof(mydata)
mydata$X <- NULL
mydata$X1 <- NULL
mydata$X2 <- NULL
mydata$X1.1 <- NULL
mydata$X2.1 <- NULL
mydata$X3 <- NULL
mydata$Rank <- NULL
mydata$City <- gsub(",.*","", mydata$City, ignore.case = TRUE)
write.csv(mydata, "mydata.csv")
library(stringr)
url <- "https://www.numbeo.com/cost-of-living/rankings.jsp"
page <- read_html(url) #Downloads the URLs webpage
table <- html_table(page, fill = TRUE)
typeof(table)
write.csv(table,"city.csv")
mydata <- read.csv(file = "city.csv", header = TRUE, sep = ",")
mydata$X1 <- NULL
mydata$X2 <- NULL
mydata$X1.1 <- NULL
mydata$X2.1 <- NULL
mydata$X3 <- NULL
mydata$Rank <- NULL
mydata$City <- gsub(",.*","", mydata$City, ignore.case = TRUE)
write.csv(mydata, "mydata.csv")
mydata$X <- NULL
write.csv(mydata, "mydata.csv")
library(twitteR)
api_key <- "l8lp8c1fYibGPEcCteHI73Y6B"
api_secret <- "XxYlM5jJ1A6eY7C1pUitRaVrYbAkrcuXH93tRDGhIopZGL7DO5"
access_token <- "267106985-iCYqablUwOsATT4AOaVLpP7BgiX53j4h1GDzmWpE"
access_token_secret <- "DfdT2CmA28ZA1XEYzY1X5vuNTKfuZs0N0gGhrPwwgwLfw"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
searchTerm <- "Munich"
# set n to the number of Tweets you want
n <- 100
#note there are many other arguments for the twitteR library
#use ?searchTwitter to view the help
tweets <- searchTwitter(searchTerm, n)
tweets <- do.call("rbind", lapply(tweets, as.data.frame))
# add in any preprocessing / cleaning you want here
# e.g.
tweets$text <- sapply(df$text,function(row) iconv(row, "latin1", "ASCII", sub=""))
# add in any filtering you want here
# e.g.
tweets <- tweets[!tweets$isRetweet, ]
tweets <- tweets[tweets$language == "en", ]
tweets <- tweets[, c("text", "created")]
# add in any transformation you want here
# e.g. the date manipulation
# strictly speaking sentiment analysis also counts as transformation
noTweets <- table(df$created)
# add in any transformation you want here
# e.g. the date manipulation
# strictly speaking sentiment analysis also counts as transformation
noTweets <- table(tweets$created)
text_df <- data_frame(date = tweets$created, text = tweets$text)
text_df <- data.frame(date = tweets$created, text = tweets$text)
text_df <- text_df %>% unnest_tokens(word, text)
??%>%
text_df <- text_df %>% unnest_tokens(word, text)
library(dplyr)
text_df <- text_df %>% unnest_tokens(word, text)
library(janeaustenr)
install.packages("janeaustenr")
library(janeaustenr)
library(janeaustenr)
text_df <- text_df %>% unnest_tokens(word, text)
sq <- read.table('http://data.princeton.edu/eco572/datasets/sweden93.dat', header = FALSE)
sq
names(sq)
names(sq) <- c("age", "p93", "L", "f")
names(sq)
sq <- mutate(sq, L= L/100000)
install.packages("mutate")
library(dplyr)
library(dplyr)
sq <- mutate(sq, L= L/100000)
sq <- mutate(sq, L= L/100000)
sq
sq <- read.table('http://data.princeton.edu/eco572/datasets/sweden93.dat', header = FALSE)
names(sq) <- c("age", "p93", "L", "f")
sq
sq <- mutate(sq, L= L/100000)
sq
library(forbesListR)
companies <- get_year_forbes_list_data(list = "Best Countries for Business", year = 2018)
library(jsonlite, stringr, dplyr, magittr)
library(jsonlite, stringr, dplyr, magrittr)
library(magrittr)
library(magrittr)
library(dplyr)
library(dplyr)
library(stringr)
library(stringr)
library(jsonlite)
library(jsonlite)
companies <- get_year_forbes_list_data(list = "Best Countries for Business", year = 2018)
companies <- get_year_forbes_list_data(list = "Best Countries for Business", year = 2017)
write.table(companies, "bestcountr.csv", sep = ",")
companies
write.table(companies, "bestcountr.csv", sep = ",")
companies <- get_year_forbes_list_data(list = "Innovative Companies", year = 2018)
write.table(companies, "innovative.csv", sep = ",")
setwd("C:/Users/Admin/Desktop/PDA")
fifa18_final = read.csv("Hive_output1.csv", stringsAsFactors = F, row.names = F)
fifa18_final = read.csv2("Hive_output1.csv", stringsAsFactors = F, row.names = F)
fifa18_final = read.csv2("Hive_output1.csv", stringsAsFactors = F, row.names = FALSE)
fifa18_final = read.csv2("Hive_output1.csv", stringsAsFactors = F)
fifa18_final = read.csv2("Hive_output1.csv", stringsAsFactors = F)
fifa18_final = read.csv("Hive_output1.csv", stringsAsFactors = F)
library(ggplot2)
ggplot(fifa18_final) +
geom_tile(aes(overall, Potential, fill = Age)) +
scale_fill_distiller(palette = "Spectral") +
theme( panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.line = element_line(colour = "black"))
ggplot(fifa18_final) +
geom_tile(aes(overall, value, fill = Age)) +
scale_fill_distiller(palette = "Spectral") +
theme( panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.line = element_line(colour = "black"))
ggplot(fifa18_final) +
geom_tile(aes(overall, value, fill = age)) +
scale_fill_distiller(palette = "Spectral") +
theme( panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.line = element_line(colour = "black"))
fifa18_final$value <- fifa18_final$value/1000
ggplot(fifa18_final) +
geom_tile(aes(overall, value, fill = age)) +
scale_fill_distiller(palette = "Spectral") +
theme( panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.line = element_line(colour = "black"))
setwd("C:/Users/Admin/Desktop/PDA/Pigout2")
fifa18_final = read.csv("part.csv", stringsAsFactors = F)
fifa18_final$value <- fifa18_final$value/1000
ggplot(fifa18_final) +
geom_tile(aes(overall, value, fill = age)) +
scale_fill_distiller(palette = "Spectral") +
theme( panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.line = element_line(colour = "black"))
ggplot(fifa18_final) +
geom_tile(aes(potetial, value, fill = preferred.position)) +
scale_fill_distiller(palette = "Spectral") +
theme( panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.line = element_line(colour = "black"))
ggplot(fifa18_final) +
geom_tile(aes(potential, value, fill = preferred.position)) +
scale_fill_distiller(palette = "Spectral") +
theme( panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.line = element_line(colour = "black"))
ggplot(fifa18_final) +
geom_tile(aes(potential, value, fill = vision)) +
scale_fill_distiller(palette = "Spectral") +
theme( panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.line = element_line(colour = "black"))
